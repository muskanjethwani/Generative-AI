{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxQZ1Etg39z_"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "gemini_api=userdata.get('gemini_key')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import generativeai as gi"
      ],
      "metadata": {
        "id": "Ip02aKlkQU7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gi.configure(api_key=gemini_api)"
      ],
      "metadata": {
        "id": "6NVYnyfEQvv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in gi.list_models():\n",
        "  print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KU_YxdagRNMh",
        "outputId": "4ccd2d76-a2ff-483e-857d-3ea9ce685cb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/embedding-gecko-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding Gecko',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=1024,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.5-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
            "                   'million tokens.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-pro-002',\n",
            "      base_model_id='',\n",
            "      version='002',\n",
            "      display_name='Gemini 1.5 Pro 002',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in September of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
            "                   'across diverse tasks.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash',\n",
            "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
            "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-002',\n",
            "      base_model_id='',\n",
            "      version='002',\n",
            "      display_name='Gemini 1.5 Flash 002',\n",
            "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in September of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B',\n",
            "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
            "                   'Flash model, released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B 001',\n",
            "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
            "                   'Flash model, released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
            "                   'released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.5-pro-preview-03-25',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-03-25',\n",
            "      display_name='Gemini 2.5 Pro Preview 03-25',\n",
            "      description='Gemini 2.5 Pro Preview 03-25',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-preview-05-20',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-05-20',\n",
            "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
            "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 2.5 Flash',\n",
            "      description=('Stable version of Gemini 2.5 Flash, our mid-size multimodal model that '\n",
            "                   'supports up to 1 million tokens, released in June of 2025.'),\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-lite-preview-06-17',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-06-17',\n",
            "      display_name='Gemini 2.5 Flash-Lite Preview 06-17',\n",
            "      description='Preview release (June 11th, 2025) of Gemini 2.5 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-pro-preview-05-06',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-05-06',\n",
            "      display_name='Gemini 2.5 Pro Preview 05-06',\n",
            "      description='Preview release (May 6th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-pro-preview-06-05',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-06-05',\n",
            "      display_name='Gemini 2.5 Pro Preview',\n",
            "      description='Preview release (June 5th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-pro',\n",
            "      base_model_id='',\n",
            "      version='2.5',\n",
            "      display_name='Gemini 2.5 Pro',\n",
            "      description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-exp',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash Experimental',\n",
            "      description='Gemini 2.0 Flash Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash',\n",
            "      description='Gemini 2.0 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-001',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash 001',\n",
            "      description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in January of 2025.'),\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-exp-image-generation',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
            "      description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-lite-001',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash-Lite 001',\n",
            "      description='Stable version of Gemini 2.0 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-lite',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash-Lite',\n",
            "      description='Gemini 2.0 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-preview-image-generation',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash Preview Image Generation',\n",
            "      description='Gemini 2.0 Flash Preview Image Generation',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
            "      base_model_id='',\n",
            "      version='preview-02-05',\n",
            "      display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
            "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-lite-preview',\n",
            "      base_model_id='',\n",
            "      version='preview-02-05',\n",
            "      display_name='Gemini 2.0 Flash-Lite Preview',\n",
            "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-pro-exp',\n",
            "      base_model_id='',\n",
            "      version='2.5-exp-03-25',\n",
            "      display_name='Gemini 2.0 Pro Experimental',\n",
            "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-pro-exp-02-05',\n",
            "      base_model_id='',\n",
            "      version='2.5-exp-03-25',\n",
            "      display_name='Gemini 2.0 Pro Experimental 02-05',\n",
            "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-exp-1206',\n",
            "      base_model_id='',\n",
            "      version='2.5-exp-03-25',\n",
            "      display_name='Gemini Experimental 1206',\n",
            "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-05-20',\n",
            "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
            "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-thinking-exp',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-05-20',\n",
            "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
            "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-05-20',\n",
            "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
            "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-preview-tts',\n",
            "      base_model_id='',\n",
            "      version='gemini-2.5-flash-exp-tts-2025-05-19',\n",
            "      display_name='Gemini 2.5 Flash Preview TTS',\n",
            "      description='Gemini 2.5 Flash Preview TTS',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=16384,\n",
            "      supported_generation_methods=['countTokens', 'generateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-pro-preview-tts',\n",
            "      base_model_id='',\n",
            "      version='gemini-2.5-pro-preview-tts-2025-05-19',\n",
            "      display_name='Gemini 2.5 Pro Preview TTS',\n",
            "      description='Gemini 2.5 Pro Preview TTS',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=16384,\n",
            "      supported_generation_methods=['countTokens', 'generateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/learnlm-2.0-flash-experimental',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='LearnLM 2.0 Flash Experimental',\n",
            "      description='LearnLM 2.0 Flash Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=32768,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3-1b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 1B',\n",
            "      description='',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3-4b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 4B',\n",
            "      description='',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3-12b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 12B',\n",
            "      description='',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3-27b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 27B',\n",
            "      description='',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3n-e4b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3n E4B',\n",
            "      description='',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3n-e2b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3n E2B',\n",
            "      description='',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-lite',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 2.5 Flash-Lite',\n",
            "      description='Stable verion of Gemini 2.5 Flash-Lite, released in July of 2025',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/embedding-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding 001',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/text-embedding-004',\n",
            "      base_model_id='',\n",
            "      version='004',\n",
            "      display_name='Text Embedding 004',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-embedding-exp-03-07',\n",
            "      base_model_id='',\n",
            "      version='exp-03-07',\n",
            "      display_name='Gemini Embedding Experimental 03-07',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-embedding-exp',\n",
            "      base_model_id='',\n",
            "      version='exp-03-07',\n",
            "      display_name='Gemini Embedding Experimental',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-embedding-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini Embedding 001',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/aqa',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Model that performs Attributed Question Answering.',\n",
            "      description=('Model trained to return answers to questions that are grounded in provided '\n",
            "                   'sources, along with estimating answerable probability.'),\n",
            "      input_token_limit=7168,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateAnswer'],\n",
            "      temperature=0.2,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=40)\n",
            "Model(name='models/imagen-3.0-generate-002',\n",
            "      base_model_id='',\n",
            "      version='002',\n",
            "      display_name='Imagen 3.0 002 model',\n",
            "      description='Vertex served Imagen 3.0 002 model',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predict'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/imagen-4.0-generate-preview-06-06',\n",
            "      base_model_id='',\n",
            "      version='01',\n",
            "      display_name='Imagen 4 (Preview)',\n",
            "      description='Vertex served Imagen 4.0 model',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predict'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/imagen-4.0-ultra-generate-preview-06-06',\n",
            "      base_model_id='',\n",
            "      version='01',\n",
            "      display_name='Imagen 4 Ultra (Preview)',\n",
            "      description='Vertex served Imagen 4.0 ultra model',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predict'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/veo-2.0-generate-001',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Veo 2',\n",
            "      description=('Vertex served Veo 2 model. Access to this model requires billing to be '\n",
            "                   'enabled on the associated Google Cloud Platform account. Please visit '\n",
            "                   'https://console.cloud.google.com/billing to enable it.'),\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predictLongRunning'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/veo-3.0-generate-preview',\n",
            "      base_model_id='',\n",
            "      version='3.0',\n",
            "      display_name='Veo 3',\n",
            "      description='Veo 3 preview.',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predictLongRunning'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/veo-3.0-fast-generate-preview',\n",
            "      base_model_id='',\n",
            "      version='3.0',\n",
            "      display_name='Veo 3 fast',\n",
            "      description='Veo 3 fast preview.',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predictLongRunning'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-2.5-flash-preview-native-audio-dialog',\n",
            "      base_model_id='',\n",
            "      version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19',\n",
            "      display_name='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
            "      description='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-exp-native-audio-thinking-dialog',\n",
            "      base_model_id='',\n",
            "      version='gemini-2.5-flash-exp-native-audio-thinking-dialog-2025-05-19',\n",
            "      display_name='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
            "      description='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-live-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 2.0 Flash 001',\n",
            "      description='Gemini 2.0 Flash 001',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-live-2.5-flash-preview',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini Live 2.5 Flash Preview',\n",
            "      description='Gemini Live 2.5 Flash Preview',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-live-preview',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 2.5 Flash Live Preview',\n",
            "      description='Gemini 2.5 Flash Live Preview',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in gi.list_models():\n",
        "  if \"generateContent\" in model.supported_generation_methods:\n",
        "    print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "odFzm-sxSAEi",
        "outputId": "1a6baddd-adbd-4526-eca4-2e5ca82bb259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-flash-lite-preview-06-17\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-2.5-flash-lite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#text to text -- pro version is for text to text\n",
        "\n",
        "t2t_model=gi.GenerativeModel(model_name=\"gemini-2.5-pro\")"
      ],
      "metadata": {
        "id": "wrfHpnK4T4TN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t2t_model.generate_content(\"What is the meaning of Muskan name?\").text\n",
        "# print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Kof2PQrpgLXU",
        "outputId": "3438c98f-cdc5-4948-e672-34656cc687ad",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Of course! The name **Muskan** has a very beautiful and direct meaning.\\n\\n### Primary Meaning\\n\\nThe name **Muskan** (Hindi: मुस्कान, Urdu: مسکان) literally means **\"smile\"** or **\"happiness\"**.\\n\\nIt is derived directly from the Hindi and Urdu word for a smile, which comes from the verb *muskurana*, meaning \"to smile.\"\\n\\n### Deeper Meaning and Associated Qualities\\n\\nBeyond its literal translation, the name Muskan evokes a wealth of positive qualities and sentiments:\\n\\n*   **Joy and Positivity:** It is a name that represents happiness, cheerfulness, and a positive outlook on life.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#image to text\n",
        "I2t_model=gi.GenerativeModel(model_name=\"gemini-2.5-pro\")\n"
      ],
      "metadata": {
        "id": "52rRBav7V0yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gemini_image_description=genai.GenerativeModel(model_name=\"gemini-2.5-pro\")"
      ],
      "metadata": {
        "id": "EhHG_gurWljR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image"
      ],
      "metadata": {
        "id": "fTSe37T6wBQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image=PIL.Image.open(\"/photo.jpg\")"
      ],
      "metadata": {
        "id": "euQeeCtOwKCN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "d5fc0d3a-a52d-4967-9d34-ab7d62d7a9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/photo.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1849844290.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/photo.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3513\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3514\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3515\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/photo.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "QDCYvIaOyXGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "I2t_model.generate_content([\"Write a short blog based on the picture\", image]).text"
      ],
      "metadata": {
        "id": "fetDZS63xLMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word embeddings (converting word into number in higher dimenssional) ex: One hot encoder\n",
        "#word to vectors ex: word 2 vectors\n"
      ],
      "metadata": {
        "id": "CGXSyepzLhot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\"Hello\", \"Generative AI is cool\"]"
      ],
      "metadata": {
        "id": "-tBzsG-YGFIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "cm_WujttHoqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess   #simple process converting sentence into tokens like into a each word)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "cEKecuXMGYHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [simple_preprocess(sentence) for sentence in corpus]"
      ],
      "metadata": {
        "id": "mT_-9Bn3JEv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "id": "B7iWS30hJWE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = [['hello', 'muskan'], ['generative', 'ai', 'is', 'cool']]\n",
        "model = Word2Vec(sentences, vector_size=64, min_count=1)\n",
        "\n",
        "# Access the vector for a word\n",
        "print(model.wv['muskan'])"
      ],
      "metadata": {
        "id": "30vCD_TsJ2xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using of langchain (reg application)\n",
        "%%capture\n",
        "!pip -q install langchain\n",
        "!pip install pypdf #pypdf too download the pdf\n",
        "!pip install sentence-transformers==2.2.2  #to convert the word into embeddings"
      ],
      "metadata": {
        "id": "QhOIvhhEcWQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install openai\n",
        "!pip install tiktoken  # convert the words into tokens\n",
        "!pip install faiss-cpu  #to enter data-base"
      ],
      "metadata": {
        "id": "fETqSifcdC_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pdfs"
      ],
      "metadata": {
        "id": "pqsgg6GWdrkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "id": "15W7A9GRKXav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader"
      ],
      "metadata": {
        "id": "SMXEB0z8JvnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader=PyPDFDirectoryLoader(\"pdfs\")"
      ],
      "metadata": {
        "id": "ZpHZjIqBKeVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "zaWlazr7Ky8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "Te4q72NlK8Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "GXymcJQvLHg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter=RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=20)"
      ],
      "metadata": {
        "id": "QRm_nTy3LaJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks=splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "9xpPZgaCLjF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "id": "AWrL_LCmL7hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "IaMkBFdPMlGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "PSjCy6l4Rcw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n"
      ],
      "metadata": {
        "id": "tF3RGPxDSHEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "OpcBHm-FNPa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "r-C7lynqSXLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordatabase = FAISS.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "2X1aj8BhSv9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is the phone munber\""
      ],
      "metadata": {
        "id": "88iO0OMQSv0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordatabase.similarity_search(query, top_k=2)"
      ],
      "metadata": {
        "id": "39Q_BKbzTZ31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "XhKzhIaWTosS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key=userdata.get('open.api_key')"
      ],
      "metadata": {
        "id": "Vn1wWf9IVMau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "rcTp77nGU3Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "j0-OPrHwVahF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=RetrievalQA.from_chain_type(llm=llm,chain_type=\"stuff\",retriever=vectordatabase.as_retriever())"
      ],
      "metadata": {
        "id": "CV271vOmVvKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"what is the collage name\""
      ],
      "metadata": {
        "id": "N5WVbnelYgiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.run(query)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1hPqPwFGX2ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain huggingface_hub transformers\n"
      ],
      "metadata": {
        "id": "5yKmdiX7kFGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n"
      ],
      "metadata": {
        "id": "tNbgpVDdkLdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-base\",\n",
        "    model_kwargs={\"temperature\": 0.5, \"max_length\": 512},\n",
        "    task=\"text2text-generation\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "8OgTRdLvkPGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-base\",\n",
        "    model_kwargs={\"temperature\": 0.5, \"max_length\": 512},\n",
        "    task=\"text2text-generation\"\n",
        ")"
      ],
      "metadata": {
        "id": "d52UFwvRkYzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet langchain huggingface_hub transformers\n",
        "\n"
      ],
      "metadata": {
        "id": "5af-H-Ewk-hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n"
      ],
      "metadata": {
        "id": "_UbaZRoulg8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "# Load pipeline directly using Transformers\n",
        "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "\n",
        "# Use with LangChain\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n"
      ],
      "metadata": {
        "id": "LhldN4EUljfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectordatabase.as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "query = \"What is the CGPA mentioned in the resume?\"\n",
        "response = qa_chain.invoke({\"query\": \"Extract and return only the CGPA value from the resume. Do not include any other text.\"})\n",
        "\n",
        "\n",
        "print(\"Answer:\", response['result'])\n",
        "print(\"\\nSources:\\n\")\n",
        "for doc in response['source_documents']:\n",
        "    print(doc.page_content[:300])\n"
      ],
      "metadata": {
        "id": "kcNTp9diltWF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}